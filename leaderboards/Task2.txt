{
    "data": [
        {
            "BD": 0.853,
            "mAP": 0.719,
            "SP": 0.662,
            "description": "We adopt two pipelines for this challenge. And the final result is a fusion version, simply by NMS, of these two pipelines. One of the pipelines is based on MASK RCNN (https://arxiv.org/abs/1703.06870). We modified the architecture of the network for training, to be exact, we adopt RPN-FRCN-Mask concatenated architecture, instead of \u2018Fast RCNN head\u2019 & \u2018Mask head\u2019 paralleling structure. And we augmented the network as PANet does (https://arxiv.org/abs/1803.01534). The other pipeline\u2019s network is same as before, but we remove \u2018Mask head\u2019 and use SLPR (https://arxiv.org/abs/1801.09969) to fit the outline of the object, then we use object\u2019s outline to generate a oriented rectangle for RoIRotate, so we adopt Cascade R-CNN as (https://arxiv.org/abs/1712.00726) with two steps, the first step we propose a horizontal rectangle, the second step we propose a oriented rectangle with SLPR, finally, we regress four quadrilateral vertices. Besides, we use some tricks for better performance, including class balance resampling, image rotation, multi-scale training & testing, model assembling. When model assembling, we use ResNet-50, ResNet101 and ResNeXt-101 as backbone network, respectively. Finally, we combine training set with validation set for training.",
            "GTF": 0.681,
            "Bridge": 0.472,
            "TeamNames": "USTC-NELSLIP",
            "BC": 0.841,
            "SV": 0.743,
            "ST": 0.765,
            "LV": 0.776,
            "Harbor": 0.642,
            "Plane": 0.903,
            "SBF": 0.674,
            "RA": 0.49,
            "HC": 0.639,
            "Ship": 0.737,
            "TC": 0.9,
            "TeamMembers": "Yixing Zhu, Chixiang Ma, Jun Du"
        },
        {
            "BD": 0.821,
            "mAP": 0.693,
            "SP": 0.644,
            "description": "We trained three object detector based on faster rcnn[1], by caffe2 deep learning framework, in parallel, on 8 GPUs Titan X. The design of our detectors follows the idea of FPN[2], whose feature extractors are resnet-50[3], resnet101, ResNeXt[6], respectively which are pre-trained on ImageNet only. To make the most of the data, we do the following for the training data: 1: We use the train + val dataset as the training data. 2: We crop the images into multiple sizes, like 1024x1024,768x768. 3: When selecting the prior boxes, we set multiple specific aspect ratios based on the scale distribution of the training data. We use soft-nms[5] instead of conventional nms to select predicted boxes. We changed RoIPooling to RoIAlign[4] to do feature quantification. In the training phase, we use multi-scale training, and when we test, we use Multi-scale testing. On the valid-set, our best single detector had obtained mAP 67.69, and the ensemble of three detectors had achieved mAP 70.2. [1] Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" NIPs. 2015.. [2] Tsung-Yi Lin, Poitr Doll\u00e1r, Ross Girshick, et al. Feature pyramid networks for object detection. CVPR 2017. [3] He, Kaiming, et al. \"Deep residual learning for image recognition.\" CVPR 2016. [4] He, Kaiming, et al. \"Mask r-cnn.\" ICCV 2017. [5] Bodla, Navaneeth, et al. \"Soft-NMS\u2014Improving Object Detection with One Line of Code.\" ICCV 2017. [6] Xie, Saining, et al. \"Aggregated residual transformations for deep neural networks.\" CVPR 2017.",
            "GTF": 0.679,
            "Bridge": 0.515,
            "TeamNames": "DeepVision-UESTC",
            "BC": 0.773,
            "SV": 0.726,
            "ST": 0.793,
            "LV": 0.738,
            "Harbor": 0.656,
            "Plane": 0.877,
            "SBF": 0.511,
            "RA": 0.51,
            "HC": 0.603,
            "Ship": 0.676,
            "TC": 0.875,
            "TeamMembers": "Hongliang LI, Qishang Cheng, Wei Li, Xiaoyu Chen, Heqian Qiu, Zichen Song, Yao Qi, Zihan Zhong"
        },
        {
            "BD": 0.698,
            "mAP": 0.638,
            "SP": 0.603,
            "description": "We used the code of Feature Pyramid Network from the project of Deformable ConvNets. The preprocess and the post process followed the method in the DevKit.\nFor example, the size of img is 1024 * 1024, the stride of spliting is 512 and the threshold of NMS is 0.3.\nWe changed the params like the num of sampled rois in order to make the alg adptive to the DOTA dataset. However, the changes are time-wasted, so we didn't make a complete change.\nBecuase of the limitation of the time, we just trained the model for 7 epochs. Similar to the changes of params, the training process is not complete, too.\nSo if the server is still open after the comp, the result may be higher.",
            "GTF": 0.551,
            "Bridge": 0.446,
            "TeamNames": "madebyrag",
            "BC": 0.715,
            "SV": 0.645,
            "ST": 0.764,
            "LV": 0.675,
            "Harbor": 0.622,
            "Plane": 0.875,
            "SBF": 0.59,
            "RA": 0.347,
            "HC": 0.439,
            "Ship": 0.701,
            "TC": 0.895,
            "TeamMembers": "Mingtao Fu, Yongchao Xu"
        },
        {
            "BD": 0.762,
            "mAP": 0.595,
            "SP": 0.58,
            "description": "This model is fine-tuned from pre-trained model on ImageNet [1] using Faster RCNN [2] with ResNet-101 [3]. All images in training and validation sets are cropped into small patches and rotated with several degrees for augmentation before training. Multi-scale detection is performed while testing for better detection of both large and small objects. Images in testing set are resized into three scales and also cropped into small patches. These patches are passed to the detector and detection results for original images are restored by combining results in small patches of the same original image together. Soft non-maximum suppresion [4] is implemented on the final restored results. [1] \u201cDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009, June). Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248-255). IEEE.\u201d http://www.image-net.org/papers/imagenet_cvpr09.pdf [2] \u201cGirshick, R. (2015, December). Fast R-CNN. In Computer Vision (ICCV), 2015 IEEE International Conference on (pp. 1440-1448). IEEE.\u201d https://arxiv.org/abs/1506.01497 [3] \u201cHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\u201d https://arxiv.org/abs/1512.03385 [4] \u201cBodla, N., Singh, B., Chellappa, R., & Davis, L. S. (2017, October). Soft-NMS\u2014Improving Object Detection with One Line of Code. In 2017 IEEE International Conference on Computer Vision (ICCV) (pp. 5562-5570). IEEE.\u201d https://arxiv.org/abs/1704.04503",
            "GTF": 0.634,
            "Bridge": 0.414,
            "TeamNames": "chx",
            "BC": 0.71,
            "SV": 0.538,
            "ST": 0.666,
            "LV": 0.559,
            "Harbor": 0.594,
            "Plane": 0.812,
            "SBF": 0.49,
            "RA": 0.34,
            "HC": 0.454,
            "Ship": 0.554,
            "TC": 0.813,
            "TeamMembers": "Xue Chuhui"
        },
        {
            "BD": 0.499,
            "mAP": 0.527,
            "SP": 0.485,
            "description": "Our group made the ODAI competition by using the faster-rcnn method on the caffe platform.First, we use the imgsplit.py file to convert the raw data to 1.5 times the width and width of the 1024*1024 data. Data analysis of the segmented data is used to determine the number of models, including the each class sample of length, aspect ratio, and so on. For example, if the sample is too large, consider using a single model. This requires the original data to be re-segmented to meet the length of the target.Then, the core of our algorithm is to change the ratios and scales of anchors in faster-rcnn. By reasonably increasing the ratios and scales and raising the IOU's threshold, the sample selected by the model is more accurate.\uff0cand the final accuracy will also increase.Of course, we also reasonably modified the parameters in config.py during this process. In addition, multi-scale tests have also played a role.",
            "GTF": 0.297,
            "Bridge": 0.296,
            "TeamNames": "yan_guoxing",
            "BC": 0.66,
            "SV": 0.708,
            "ST": 0.557,
            "LV": 0.625,
            "Harbor": 0.475,
            "Plane": 0.794,
            "SBF": 0.383,
            "RA": 0.341,
            "HC": 0.448,
            "Ship": 0.605,
            "TC": 0.727,
            "TeamMembers": "guoxing yan, yantian wang, cong ji"
        },
        {
            "BD": 0.643,
            "mAP": 0.521,
            "SP": 0.408,
            "description": "test",
            "GTF": 0.473,
            "Bridge": 0.28,
            "TeamNames": "xdrao",
            "BC": 0.615,
            "SV": 0.493,
            "ST": 0.598,
            "LV": 0.639,
            "Harbor": 0.471,
            "Plane": 0.735,
            "SBF": 0.39,
            "RA": 0.375,
            "HC": 0.314,
            "Ship": 0.571,
            "TC": 0.815,
            "TeamMembers": "Xudong Rao"
        },
        {
            "BD": 0.505,
            "mAP": 0.518,
            "SP": 0.562,
            "description": "This method is obtained by improving the Faster RCNN. In order to enable the model to detect targets of different sizes, this method uses 27 different anchors with 9 different sizes (32,96,128,192,384,512,768,1536,3072) and 3 scales (1:1, 1:2, 2:1). When training the data, the reverse sequence method is used to cut the pictures on the edge to make full use of the training data set. The same method is used when testing the data to reduce missed detections. When testing the data, the images are cut by overlapping between the pictures, missed detections caused by dividing the same target into two pictures are reduced in this way. For the initial detection results, the non_maximum suppression method with a threshold of 0.5 is used to screen the results and get the final result.",
            "GTF": 0.367,
            "Bridge": 0.343,
            "TeamNames": "CongLin",
            "BC": 0.566,
            "SV": 0.514,
            "ST": 0.665,
            "LV": 0.674,
            "Harbor": 0.393,
            "Plane": 0.793,
            "SBF": 0.191,
            "RA": 0.339,
            "HC": 0.334,
            "Ship": 0.722,
            "TC": 0.809,
            "TeamMembers": "CongLin, ZhangXu"
        }
    ]
}